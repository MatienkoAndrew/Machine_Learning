Gradient Boosting
---

### Введение

Построение композиции — важный подход в машинном обучении, который позволяет объединять большое количество слабых алгоритмов в один сильный. Данный подход широко используется на практике в самых разных задачах.

***Метод градиентного бустинга*** последовательно строит композицию алгоритмов, причем каждый следующий алгоритм выбирается так, чтобы исправлять ошибки уже имеющейся композиции. Обычно в качестве базовых алгоритмов используют деревья небольшой глубины, поскольку их достаточно легко строить, и при этом они дают нелинейные разделяющие поверхности.
Другой метод построения композиций — случайный лес. В нем, в отличие от градиентного бустинга, отдельные деревья строятся независимо и без каких-либо ограничений на глубину — дерево наращивается до тех пор, пока не покажет наилучшее качество на обучающей выборке.
В этом задании мы будем иметь дело с задачей классификации. В качестве функции потерь будем использовать log-loss:

![Безымянный](https://user-images.githubusercontent.com/29499863/77454071-f4ce6600-6def-11ea-9bbb-c80dc863565e.png)

Здесь через y обозначен истинный ответ, через z — прогноз алгоритма. Данная функция является дифференцируемой, и поэтому подходит для использования в градиентном бустинге. Также можно показать, что при ее использовании итоговый алгоритм будет приближать истинные вероятности классов.

#### Реализация в sklearn

В пакете scikit-learn градиентный 
бустинг реализован в модуле ```ensemble```
в виде классов [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
 и [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html). 
Основные параметры: ```n_estimators, learning_rate```. 
Иногда может быть полезен параметр ```verbose``` для отслеживания процесса обучения.

Чтобы была возможность оценить качество построенной композиции на каждой итерации, у класса есть метод [staged_decision_function](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.staged_decision_function).
Для заданной выборки он возвращает ответ на каждой итерации.

Метрика [log-loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)
реализована в пакете metrics: sklearn.metrics.log_loss. Заметим, что данная метрика 
предназначена для классификаторов, 
выдающих оценку принадлежности классу, а не бинарные ответы. 
И градиентный бустинг, и случайный лес умеют строить такие прогнозы — 
для этого нужно использовать метод predict_proba:

```pred = clf.predict_proba(X_test)```

Метод predict_proba возвращает матрицу, i-й столбец которой содержит оценки принадлежности i-му классу. 

Для рисования кривых качества на обучении и контроле можно воспользоваться следующим кодом:

```
import matplotlib.pyplot as plt
%matplotlib inline
plt.figure()
plt.plot(test_loss, 'r', linewidth=2)
plt.plot(train_loss, 'g', linewidth=2)
plt.legend(['test', 'train'])
```

#### Материалы

[Подробнее о градиентном бустинге и особенностях его применения к деревьям](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture09-ensembles.pdf)